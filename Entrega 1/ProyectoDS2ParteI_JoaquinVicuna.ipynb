{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMCo_pAHmjNS"
   },
   "source": [
    "# Proyecto DS Parte I - Joaquín Vicuña Vicencio\n",
    "## Contexto:\n",
    "### En Chile, al ser un país sismico, existe una creencia que por los ultimos eventos de gran magnitud, es normal que ocurran en horarios no hábiles. Por otro lado existe un vicio de lenguaje en el cual se hace la distinción de aquellos sismos que son destructivos denominandolos \"Terremotos\" de los sismos que no son destructivos, a estos ultimos se les llama \"Temblores\".\n",
    "\n",
    "\n",
    "\n",
    "#### 1) Desarrollar un modelo de predicción para determinar la probabilidad de que futuros terremotos (sismos de magnitud ≥ 7M) en Chile ocurran durante horarios hábiles o inhábiles. El análisis se basará en datos históricos de sismos con magnitud > 4.5 registrados entre el 01-01-1950 y el 24-02-2025. Buscaremos si una prediccion asi es viable o no probando distintos modelos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASO 1: Cargando datos...\n",
      "Datos cargados: 15958 filas, 22 columnas\n",
      "\n",
      "PASO 2: Preprocesamiento inicial, target y feature engineering...\n",
      "\n",
      "NaNs por columna en X (features finales):\n",
      "mag            0\n",
      "depth          0\n",
      "latitude       0\n",
      "longitude      0\n",
      "intensidad     0\n",
      "zona_riesgo    0\n",
      "trimestre      0\n",
      "mes            0\n",
      "dtype: int64\n",
      "\n",
      "PASO 3: Visualización exploratoria (Distribución de features vs Target)...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 119\u001b[39m\n\u001b[32m    116\u001b[39m features_to_plot_num = [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features_to_plot_num \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m X.columns]\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features_to_plot_num:\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[43mplt\u001b[49m.figure(figsize=(\u001b[32m15\u001b[39m, \u001b[38;5;28mlen\u001b[39m(features_to_plot_num) * \u001b[32m2.5\u001b[39m))\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(features_to_plot_num):\n\u001b[32m    121\u001b[39m         plt.subplot(\u001b[38;5;28mlen\u001b[39m(features_to_plot_num), \u001b[32m1\u001b[39m, i+\u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, time\n",
    "import pytz\n",
    "from workalendar.america import Chile # Para manejar días hábiles en Chile\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# --- Sklearn ---\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel, RFECV, SelectKBest, f_classif\n",
    "from sklearn.metrics import (\n",
    "    classification_report, ConfusionMatrixDisplay, roc_auc_score,\n",
    "    precision_recall_curve, f1_score, roc_curve, auc, make_scorer\n",
    ")\n",
    "from sklearn.exceptions import NotFittedError # Importar NotFittedError\n",
    "\n",
    "# --- Imbalanced-learn ---\n",
    "try:\n",
    "    from imblearn.under_sampling import TomekLinks\n",
    "except ImportError:\n",
    "    print(\"Instalando imbalanced-learn...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"imbalanced-learn\"])\n",
    "        from imblearn.under_sampling import TomekLinks\n",
    "    except Exception as e:\n",
    "        print(f\"Error instalando imbalanced-learn: {e}\")\n",
    "        sys.exit(\"Se requiere imbalanced-learn. Por favor, instálalo manualmente.\")\n",
    "\n",
    "# --- Joblib ---\n",
    "from joblib import dump, load\n",
    "\n",
    "# --- Configuraciones ---\n",
    "# Suprimir warnings comunes para una salida más limpia \n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')\n",
    "# Configurar estilo de gráficos\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 1. Carga de Datos ---\n",
    "# ==============================================================================\n",
    "print(\"PASO 1: Cargando datos...\")\n",
    "url_df_github = \"https://raw.githubusercontent.com/joaco2v/ProyectoDS-JoaquinVicuna/refs/heads/main/sismos_chile.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(url_df_github)\n",
    "    print(f\"Datos cargados: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar los datos desde GitHub: {e}\")\n",
    "    sys.exit(\"No se pudieron cargar los datos.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 2. Preprocesamiento Inicial y Creación de Target / Features ---\n",
    "# ==============================================================================\n",
    "print(\"\\nPASO 2: Preprocesamiento inicial, target y feature engineering...\")\n",
    "df[\"Fecha UTC\"] = pd.to_datetime(df[\"time\"], utc=True)\n",
    "santiago_tz = pytz.timezone(\"America/Santiago\")\n",
    "df[\"Fecha UTC CL\"] = df[\"Fecha UTC\"].dt.tz_convert(santiago_tz)\n",
    "\n",
    "# --- Definición Variable Objetivo (Target) ---\n",
    "cal = Chile()\n",
    "def es_habil(fecha_hora):\n",
    "    fecha = fecha_hora.date()\n",
    "    hora = fecha_hora.time()\n",
    "    dia_semana = fecha_hora.weekday()\n",
    "    if cal.is_holiday(fecha): return \"No habil\"\n",
    "    if dia_semana >= 5: return \"No habil\" # Sábado (5) y Domingo (6)\n",
    "    # Horario hábil: Lunes a Viernes, 7:30 a 19:30\n",
    "    return \"Habil\" if time(7, 30) <= hora <= time(19, 30) else \"No habil\"\n",
    "\n",
    "df[\"Horario_Habil\"] = df[\"Fecha UTC CL\"].apply(es_habil)\n",
    "df['Horario_Habil_bin'] = df['Horario_Habil'].eq('Habil').astype(int) # Target binario\n",
    "\n",
    "# --- Feature Engineering (Variables predictoras POTENCIALES) ---\n",
    "# Se excluyen variables que definen directamente el target para evitar fuga de datos\n",
    "df['trimestre'] = df[\"Fecha UTC CL\"].dt.quarter\n",
    "df['mes'] = df[\"Fecha UTC CL\"].dt.month\n",
    "df['intensidad'] = df['mag'] * (1/(df['depth'].clip(lower=0) + 1))\n",
    "lat_bins = [-90, -30, -20, -10, 90]\n",
    "lat_labels = ['Austral', 'Sur', 'Central', 'Norte']\n",
    "df['zona_riesgo'] = pd.cut(df['latitude'], bins=lat_bins, labels=lat_labels, right=False)\n",
    "df['zona_riesgo'] = df['zona_riesgo'].cat.add_categories('Desconocida').fillna('Desconocida')\n",
    "\n",
    "# --- Lista Final de Features (Predictoras) SIN FUGA DE DATOS ---\n",
    "features = [\n",
    "    'mag', 'depth', 'latitude', 'longitude', 'intensidad',\n",
    "    'zona_riesgo', 'trimestre', 'mes'\n",
    "]\n",
    "target = 'Horario_Habil_bin' # Usar la columna binaria creada\n",
    "\n",
    "X = df[features].copy()\n",
    "y = df[target].copy()\n",
    "\n",
    "# Verificar NaNs en features seleccionadas\n",
    "print(\"\\nNaNs por columna en X (features finales):\")\n",
    "nan_check = X.isnull().sum()\n",
    "print(nan_check)\n",
    "if nan_check.sum() > 0:\n",
    "    print(\"¡ADVERTENCIA! Se encontraron NaNs en las features. Se necesita imputación.\")\n",
    "    # Aquí iría la lógica de imputación si fuera necesaria\n",
    "    # Para este ejemplo, asumiendo que no hay NaNs en las features finales como en el notebook original.\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 3. Visualización Exploratoria (EDA) ---\n",
    "# ==============================================================================\n",
    "print(\"\\nPASO 3: Visualización exploratoria (Distribución de features vs Target)...\")\n",
    "\n",
    "# Seleccionar algunas features numéricas clave para visualizar\n",
    "features_to_plot_num = ['mag', 'depth', 'intensidad', 'latitude', 'longitude']\n",
    "features_to_plot_num = [f for f in features_to_plot_num if f in X.columns]\n",
    "\n",
    "if features_to_plot_num:\n",
    "    plt.figure(figsize=(15, len(features_to_plot_num) * 2.5))\n",
    "    for i, col in enumerate(features_to_plot_num):\n",
    "        plt.subplot(len(features_to_plot_num), 1, i+1)\n",
    "        sns.violinplot(x=y, y=X[col], palette=\"muted\")\n",
    "        plt.title(f'Distribución de {col} por Horario Hábil (0=No, 1=Sí)')\n",
    "        plt.xlabel(\"Horario Hábil\")\n",
    "        plt.ylabel(col)\n",
    "    plt.suptitle(\"Distribución de Features Numéricas por Clase Objetivo\", y=1.02, fontsize=14)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n",
    "    plt.savefig('num_features_distribution.png')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No se encontraron features numéricas clave para graficar.\")\n",
    "\n",
    "# Visualizar feature categórica (zona_riesgo)\n",
    "# if 'zona_riesgo' in X.columns:\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     zona_target_counts = df.groupby(['zona_riesgo', target]).size().unstack(fill_value=0)\n",
    "#     zona_target_perc = zona_target_counts.apply(lambda x: x / x.sum() * 100, axis=1)\n",
    "#     zona_target_perc.plot(kind='bar', stacked=True, colormap='coolwarm', ax=plt.gca())\n",
    "#     plt.title('Distribución Porcentual Horario Hábil/No Hábil por Zona de Riesgo')\n",
    "#     plt.ylabel('Porcentaje (%)')\n",
    "#     plt.xlabel('Zona de Riesgo')\n",
    "#     plt.xticks(rotation=45, ha='right')\n",
    "#     plt.legend(title='Horario Hábil', labels=['No Habil (0)', 'Habil (1)'])\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig('zona_riesgo_distribution.png')\n",
    "#     plt.show()\n",
    "# else:\n",
    "#     print(\"No se encontró la feature 'zona_riesgo' para graficar.\")\n",
    "\n",
    "\n",
    "# --- Nueva sección: Análisis de 'Temblor' vs 'Terremoto' (Coloquial) ---\n",
    "print(\"\\nAnalizando la distinción coloquial 'Temblor' vs 'Terremoto'...\")\n",
    "\n",
    "# Crear la columna 'tipo_sismo_coloquial'\n",
    "# Asumimos que 'Terremoto' es mag >= 7M y 'Temblor' es mag < 7M\n",
    "df['tipo_sismo_coloquial'] = np.where(df['mag'] >= 7.0, 'Terremoto', 'Temblor')\n",
    "\n",
    "# --- Gráfico 1: Temblores en Horario Hábil y No Hábil ---\n",
    "temblor_df = df[df['tipo_sismo_coloquial'] == 'Temblor'].copy()\n",
    "temblor_counts = temblor_df['Horario_Habil_bin'].value_counts(normalize=True) * 100\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.barplot(x=temblor_counts.index, y=temblor_counts.values, palette='coolwarm')\n",
    "plt.title('Proporción de \"Temblores\" (Mag < 7.0M) por Horario')\n",
    "plt.xlabel('Horario (0: No Hábil, 1: Hábil)')\n",
    "plt.ylabel('Porcentaje (%)')\n",
    "plt.xticks(ticks=[0, 1], labels=['No Hábil', 'Hábil'])\n",
    "plt.ylim(0, 100)\n",
    "for index, value in enumerate(temblor_counts.values):\n",
    "    plt.text(index, value + 2, f'{value:.2f}%', ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.savefig('temblores_horario.png')\n",
    "plt.show()\n",
    "\n",
    "# --- Gráfico 2: Terremotos en Horario Hábil y No Hábil ---\n",
    "terremoto_df = df[df['tipo_sismo_coloquial'] == 'Terremoto'].copy()\n",
    "terremoto_counts = terremoto_df['Horario_Habil_bin'].value_counts(normalize=True) * 100\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.barplot(x=terremoto_counts.index, y=terremoto_counts.values, palette='viridis')\n",
    "plt.title('Proporción de \"Terremotos\" (Mag ≥ 7.0M) por Horario')\n",
    "plt.xlabel('Horario (0: No Hábil, 1: Hábil)')\n",
    "plt.ylabel('Porcentaje (%)')\n",
    "plt.xticks(ticks=[0, 1], labels=['No Hábil', 'Hábil'])\n",
    "plt.ylim(0, 100)\n",
    "for index, value in enumerate(terremoto_counts.values):\n",
    "    plt.text(index, value + 2, f'{value:.2f}%', ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.savefig('terremotos_horario.png')\n",
    "plt.show()\n",
    "\n",
    "# --- Gráfico 3: Periodicidad de Terremotos (Magnitud ≥ 7.0M) ---\n",
    "# Filtrar solo por terremotos y ordenar por fecha\n",
    "terremotos_fechas = df[df['mag'] >= 7.0].sort_values('Fecha UTC CL')['Fecha UTC CL']\n",
    "\n",
    "# Extraer el año para un análisis de frecuencia anual\n",
    "terremotos_anuales = terremotos_fechas.dt.year.value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=terremotos_anuales.index, y=terremotos_anuales.values, marker='o')\n",
    "plt.title('Frecuencia Anual de \"Terremotos\" (Mag ≥ 7.0M) en Chile')\n",
    "plt.xlabel('Año')\n",
    "plt.ylabel('Número de Terremotos')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('terremotos_anual.png')\n",
    "plt.show()\n",
    "\n",
    "# Opcional: Histograma de diferencias de tiempo entre terremotos (para ver si hay patrones de recurrencia en días/meses)\n",
    "# Calcular la diferencia de tiempo entre eventos consecutivos\n",
    "if len(terremotos_fechas) > 1:\n",
    "    time_diffs = terremotos_fechas.diff().dropna().dt.days # Diferencia en días\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(time_diffs, bins=50, kde=True)\n",
    "    plt.title('Distribución de Días entre \"Terremotos\" Consecutivos (Mag ≥ 7.0M)')\n",
    "    plt.xlabel('Días entre Terremotos')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.yscale('log') # Escala logarítmica para ver colas largas\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('terremotos_time_diff_hist.png')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No hay suficientes 'Terremotos' para calcular diferencias de tiempo.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 4. División de Datos (Entrenamiento y Prueba) ---\n",
    "# ==============================================================================\n",
    "print(\"\\nPASO 4: Dividiendo datos en entrenamiento y prueba...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "print(f\"Datos divididos: X_train={X_train.shape}, X_test={X_test.shape}, y_train={y_train.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 5. Preprocesamiento (Escalado y Codificación) ---\n",
    "# ==============================================================================\n",
    "print(\"\\nPASO 5: Definiendo y aplicando preprocesamiento...\")\n",
    "numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "if 'zona_riesgo' not in categorical_features and 'zona_riesgo' in X_train.columns:\n",
    "    categorical_features.append('zona_riesgo')\n",
    "    if 'zona_riesgo' in numeric_features: numeric_features.remove('zona_riesgo')\n",
    "\n",
    "\n",
    "transformers_list = []\n",
    "if numeric_features:\n",
    "    transformers_list.append(('num', RobustScaler(), numeric_features))\n",
    "    print(f\"Aplicando RobustScaler a: {numeric_features}\")\n",
    "if categorical_features:\n",
    "    transformers_list.append(('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features))\n",
    "    print(f\"Aplicando OneHotEncoder a: {categorical_features}\")\n",
    "\n",
    "if not transformers_list:\n",
    "     raise ValueError(\"No se definieron transformadores. Verifica las listas de features numéricas/categóricas.\")\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=transformers_list, remainder='drop')\n",
    "\n",
    "# Ajustar y transformar\n",
    "preprocessor.fit(X_train)\n",
    "X_train_processed = preprocessor.transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Obtener nombres de features procesadas\n",
    "try:\n",
    "    feature_names_processed = preprocessor.get_feature_names_out()\n",
    "    print(f\"Número de features después del preprocesamiento: {len(feature_names_processed)}\")\n",
    "except Exception as e:\n",
    "    print(f\"No se pudieron obtener nombres de features procesadas: {e}\")\n",
    "    feature_names_processed = np.array([f\"feat_{i}\" for i in range(X_train_processed.shape[1])])\n",
    "\n",
    "print(f\"Dimensiones X_train procesado: {X_train_processed.shape}\")\n",
    "print(f\"Dimensiones X_test procesado: {X_test_processed.shape}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 6. Balanceo de Clases (Resampling) ---\n",
    "# ==============================================================================\n",
    "print(\"\\nPASO 6: Aplicando resampling (Tomek Links por defecto)...\")\n",
    "\n",
    "resampler = TomekLinks(sampling_strategy='majority', n_jobs=-1)\n",
    "\n",
    "print(f\"Usando resampler: {type(resampler).__name__}\")\n",
    "print(\"\\nBalance de clases en y_train ANTES de resampling:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "X_train_resampled_processed, y_train_resampled = resampler.fit_resample(X_train_processed, y_train)\n",
    "\n",
    "print(\"\\nBalance de clases en y_train DESPUÉS de resampling:\")\n",
    "print(pd.Series(y_train_resampled).value_counts(normalize=True))\n",
    "print(f\"Dimensiones X_train después de resampling: {X_train_resampled_processed.shape}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 7. Función de Evaluación Detallada ---\n",
    "# ==============================================================================\n",
    "def evaluate_model_detailed(pipeline, X_test_processed, y_test, model_name=\"Modelo\"):\n",
    "    \"\"\"\n",
    "    Evalúa un pipeline (selector + clasificador) o un clasificador ya entrenado,\n",
    "    generando reporte, matriz de confusión, curva ROC y curva Precision-Recall.\n",
    "    Retorna AUC y F1-score en el umbral óptimo.\n",
    "    \"\"\"\n",
    "    print(f\"\\n===== Evaluación Detallada: {model_name} =====\")\n",
    "    try:\n",
    "        y_probs = pipeline.predict_proba(X_test_processed)[:, 1]\n",
    "    except NotFittedError as e:\n",
    "        print(f\"Error: El modelo/pipeline '{model_name}' no parece estar ajustado. {e}\")\n",
    "        return 0.0, 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"Error durante predict_proba para '{model_name}': {e}\")\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(y_test, y_probs)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "    f1_scores = np.nan_to_num(f1_scores)\n",
    "\n",
    "    if len(f1_scores) > 0:\n",
    "        idx_opt_f1 = np.argmax(f1_scores)\n",
    "        if idx_opt_f1 > 0 and (idx_opt_f1 - 1) < len(pr_thresholds):\n",
    "            optimal_threshold = pr_thresholds[idx_opt_f1 - 1]\n",
    "        else:\n",
    "            optimal_threshold = 0.5\n",
    "        optimal_f1 = f1_scores[idx_opt_f1]\n",
    "        print(f\"Umbral óptimo (max F1) encontrado: {optimal_threshold:.4f} (F1 = {optimal_f1:.4f})\")\n",
    "    else:\n",
    "        print(\"Advertencia: No se pudieron calcular F1 scores. Usando umbral 0.5.\")\n",
    "        optimal_threshold = 0.5\n",
    "        optimal_f1 = 0\n",
    "\n",
    "    y_pred_opt = (y_probs >= optimal_threshold).astype(int)\n",
    "\n",
    "    report = classification_report(y_test, y_pred_opt, target_names=['No Habil (0)', 'Habil (1)'], zero_division=0)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_test, y_probs)\n",
    "    except ValueError:\n",
    "        print(\"Advertencia: ROC AUC no se puede calcular (posiblemente solo una clase presente en y_true o y_score).\")\n",
    "        roc_auc = 0.0\n",
    "    print(\"\\n--- Reporte de Clasificación (Umbral Óptimo) ---\")\n",
    "    print(report)\n",
    "    print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(22, 6))\n",
    "    fig.suptitle(f'Evaluación: {model_name} (Umbral F1 Óptimo ~ {optimal_threshold:.3f})', fontsize=16, y=1.02)\n",
    "\n",
    "    try:\n",
    "        ConfusionMatrixDisplay.from_predictions(y_test, y_pred_opt, display_labels=['No Habil', 'Habil'], normalize='true', cmap='Blues', ax=axes[0], values_format=\".2%\")\n",
    "        axes[0].set_title('Matriz de Confusión (Normalizada)'); axes[0].grid(False)\n",
    "    except Exception as e: axes[0].set_title(f'Error CM:\\n{e}')\n",
    "\n",
    "    try:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
    "        roc_auc_val = auc(fpr, tpr)\n",
    "        axes[1].plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (AUC = {roc_auc_val:.4f})')\n",
    "        axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Azar')\n",
    "        axes[1].set_xlabel('Tasa de Falsos Positivos (FPR)'); axes[1].set_ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
    "        axes[1].set_title('Curva ROC'); axes[1].legend(loc=\"lower right\"); axes[1].grid(True)\n",
    "    except Exception as e: axes[1].set_title(f'Error ROC:\\n{e}')\n",
    "\n",
    "    try:\n",
    "        axes[2].plot(recall, precision, color='blue', lw=2, label='Curva Precision-Recall')\n",
    "        if idx_opt_f1 < len(recall) and idx_opt_f1 < len(precision):\n",
    "           axes[2].scatter(recall[idx_opt_f1], precision[idx_opt_f1], marker='o', color='red', s=100, zorder=5, label=f'Umbral Óptimo (F1={optimal_f1:.3f})')\n",
    "        axes[2].set_xlabel('Recall (Sensibilidad)'); axes[2].set_ylabel('Precision')\n",
    "        axes[2].set_ylim([0.0, 1.05]); axes[2].set_xlim([0.0, 1.0])\n",
    "        axes[2].set_title('Curva Precision-Recall'); axes[2].legend(loc=\"best\"); axes[2].grid(True)\n",
    "    except Exception as e: axes[2].set_title(f'Error P-R:\\n{e}')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    return roc_auc, optimal_f1\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 8. Ejecución de Evaluaciones y Comparación ---\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PASO 8: INICIANDO EVALUACIÓN DE MODELOS (CON FEATURES CORREGIDAS)\")\n",
    "print(\"        (Resultados reflejarán la predictibilidad real, probablemente baja)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "results_summary = {}\n",
    "num_processed_features = X_train_resampled_processed.shape[1]\n",
    "\n",
    "# --- 1. Modelo Base (Random Forest) ---\n",
    "print(\"\\n--- Evaluando: RF Base ---\")\n",
    "base_rf = RandomForestClassifier(class_weight='balanced', random_state=42, n_estimators=100, n_jobs=-1)\n",
    "base_rf.fit(X_train_resampled_processed, y_train_resampled)\n",
    "auc_base, f1_base = evaluate_model_detailed(base_rf, X_test_processed, y_test, model_name=\"RF Base\")\n",
    "results_summary[\"RF Base\"] = {'AUC': auc_base, 'F1_Opt': f1_base, 'Num_Features': num_processed_features}\n",
    "\n",
    "# --- 2. Selección con SelectFromModel (usando RF) ---\n",
    "print(\"\\n--- Evaluando: SFM(RF)+RF ---\")\n",
    "selector_estimator_sfm = RandomForestClassifier(n_estimators=50, class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "pipeline_sfm_rf = Pipeline([\n",
    "    ('feature_selection', SelectFromModel(selector_estimator_sfm, threshold='median')),\n",
    "    ('classifier', RandomForestClassifier(class_weight='balanced', random_state=42, n_estimators=100, n_jobs=-1))\n",
    "])\n",
    "pipeline_sfm_rf.fit(X_train_resampled_processed, y_train_resampled)\n",
    "auc_sfm, f1_sfm = evaluate_model_detailed(pipeline_sfm_rf, X_test_processed, y_test, model_name=\"SFM(RF)+RF\")\n",
    "try:\n",
    "    selector_sfm_rf_fitted = pipeline_sfm_rf.named_steps['feature_selection']\n",
    "    selected_features_sfm_mask = selector_sfm_rf_fitted.get_support()\n",
    "    selected_features_sfm = feature_names_processed[selected_features_sfm_mask]\n",
    "    num_sel_sfm = len(selected_features_sfm)\n",
    "    print(f\"Features seleccionadas por SFM: {num_sel_sfm} - {selected_features_sfm.tolist()}\")\n",
    "except IndexError:\n",
    "     num_sel_sfm = selected_features_sfm_mask.sum()\n",
    "     print(f\"Features seleccionadas por SFM: {num_sel_sfm} (nombres no disponibles/inconsistentes)\")\n",
    "results_summary[\"SFM(RF)+RF\"] = {'AUC': auc_sfm, 'F1_Opt': f1_sfm, 'Num_Features': num_sel_sfm}\n",
    "\n",
    "\n",
    "# --- 3. Selección con RFECV ---\n",
    "print(\"\\n--- Evaluando: RFECV(RF)+RF ---\")\n",
    "f1_scorer_pos = make_scorer(f1_score, pos_label=1, zero_division=0)\n",
    "estimator_rfe = RandomForestClassifier(n_estimators=50, class_weight='balanced', random_state=42)\n",
    "pipeline_rfecv_rf = Pipeline([\n",
    "    ('feature_selection', RFECV(\n",
    "        estimator=estimator_rfe, scoring=f1_scorer_pos, cv=StratifiedKFold(3),\n",
    "        step=1, n_jobs=-1, min_features_to_select=max(1, num_processed_features // 3)\n",
    "        )),\n",
    "    ('classifier', RandomForestClassifier(class_weight='balanced', random_state=42, n_estimators=100, n_jobs=-1))\n",
    "])\n",
    "pipeline_rfecv_rf.fit(X_train_resampled_processed, y_train_resampled)\n",
    "auc_rfecv, f1_rfecv = evaluate_model_detailed(pipeline_rfecv_rf, X_test_processed, y_test, model_name=\"RFECV(RF)+RF\")\n",
    "try:\n",
    "    selector_rfecv_fitted = pipeline_rfecv_rf.named_steps['feature_selection']\n",
    "    selected_features_rfecv_mask = selector_rfecv_fitted.support_\n",
    "    selected_features_rfecv = feature_names_processed[selected_features_rfecv_mask]\n",
    "    num_sel_rfecv = selector_rfecv_fitted.n_features_\n",
    "    print(f\"\\nFeatures seleccionadas por RFECV: {num_sel_rfecv} - {selected_features_rfecv.tolist()}\")\n",
    "except IndexError:\n",
    "    num_sel_rfecv = selected_features_rfecv_mask.sum()\n",
    "    print(f\"\\nFeatures seleccionadas por RFECV: {num_sel_rfecv} (nombres no disponibles/inconsistentes)\")\n",
    "results_summary[\"RFECV(RF)+RF\"] = {'AUC': auc_rfecv, 'F1_Opt': f1_rfecv, 'Num_Features': num_sel_rfecv}\n",
    "\n",
    "\n",
    "# --- 4. Selección con SelectKBest (ANOVA F-value) ---\n",
    "k_best = min(5, num_processed_features)\n",
    "print(f\"\\n--- Evaluando: KBest(F-val, k={k_best})+RF ---\")\n",
    "if k_best > 0:\n",
    "    pipeline_kbest_rf = Pipeline([\n",
    "        ('feature_selection', SelectKBest(score_func=f_classif, k=k_best)),\n",
    "        ('classifier', RandomForestClassifier(class_weight='balanced', random_state=42, n_estimators=100, n_jobs=-1))\n",
    "    ])\n",
    "    pipeline_kbest_rf.fit(X_train_resampled_processed, y_train_resampled)\n",
    "    auc_kbest, f1_kbest = evaluate_model_detailed(pipeline_kbest_rf, X_test_processed, y_test, model_name=f\"KBest(k={k_best})+RF\")\n",
    "    try:\n",
    "        selector_kbest_fitted = pipeline_kbest_rf.named_steps['feature_selection']\n",
    "        selected_features_kbest_mask = selector_kbest_fitted.get_support()\n",
    "        selected_features_kbest = feature_names_processed[selected_features_kbest_mask]\n",
    "        num_sel_kbest = len(selected_features_kbest)\n",
    "        print(f\"\\nFeatures seleccionadas por SelectKBest: {num_sel_kbest} - {selected_features_kbest.tolist()}\")\n",
    "    except IndexError:\n",
    "        num_sel_kbest = selected_features_kbest_mask.sum()\n",
    "        print(f\"\\nFeatures seleccionadas por SelectKBest: {num_sel_kbest} (nombres no disponibles/inconsistentes)\")\n",
    "    results_summary[f\"KBest(k={k_best})+RF\"] = {'AUC': auc_kbest, 'F1_Opt': f1_kbest, 'Num_Features': num_sel_kbest}\n",
    "else:\n",
    "    print(f\"Skipping SelectKBest porque k={k_best} no es válido.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xe1s62j7raLS"
   },
   "source": [
    "#### --- Justificación del Modelo Final (Resultados Realistas) ---\n",
    "El modelo 'RFECV(RF)+RF' se selecciona como el 'mejor' relativo entre los evaluados.\n",
    "Criterios de selección (ordenados por F1 desc, AUC desc, menos features):\n",
    "  - F1-score (clase 'Habil', umbral óptimo): 0.4990\n",
    "  - Área Bajo la Curva ROC (AUC):           0.5401\n",
    "  - Número de features utilizadas:          9\n",
    "\n",
    "Interpretación del Rendimiento:\n",
    "El rendimiento general es BAJO (AUC cercano a 0.5, F1 bajo).\n",
    "   Esto sugiere fuertemente que las features utilizadas (características físicas\n",
    "   del sismo y temporalidad general) NO tienen una capacidad predictiva\n",
    "   significativa para determinar si el sismo ocurre en horario hábil.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
